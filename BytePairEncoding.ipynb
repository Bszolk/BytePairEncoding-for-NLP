{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3sY2gWjZC5k9kjSa4eLeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bszolk/BytePairEncoding-for-NLP/blob/main/BytePairEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BytePairEncoding():\n",
        "    def __init__(self):\n",
        "        self.vocab: list[str] = ['<s>'] # <s> - special token representing start of the word\n",
        "        self.merge_rule: dict = {}\n",
        "        self.stoi: dict = {} # mapping from token (str) to int\n",
        "        self.itos: dict = {} # mapping from int to token (str)\n",
        "\n",
        "    def gen_vocab(self, corpus: list[str], vocab_size: int):\n",
        "        '''\n",
        "        Based on a frequency of characters in the corpus, generates a vocabulary\n",
        "        of all available tokens and a set of rules used to encode plain text\n",
        "        into these tokens.\n",
        "\n",
        "            Parameters:\n",
        "                corpus (list[str]): list of text sequences\n",
        "                vocab_size (int): method ends when given vocab_size is reached\n",
        "        '''\n",
        "        [[c not in self.vocab and self.vocab.append(c) for c in seq] for seq in corpus]\n",
        "        corpus = \" \".join(corpus)\n",
        "        corpus = [list(word) for word in corpus.split(\" \")]\n",
        "        [word.insert(0, self.vocab[0]) for word in corpus]\n",
        "\n",
        "        # find most frequent pair of tokens and merge them together\n",
        "        while len(self.vocab) < vocab_size:\n",
        "            pair_freq = {}\n",
        "            for word in corpus:\n",
        "                for i in range(len(word) - 1):\n",
        "                    pair = (word[i], word[i+1])\n",
        "                    pair_freq[pair] = pair_freq.setdefault(pair, 0) + 1\n",
        "\n",
        "            max_pair = max(pair_freq, key=pair_freq.get)\n",
        "            pair = max_pair[0] + max_pair[1]\n",
        "            self.vocab.append(pair)\n",
        "            self.merge_rule[max_pair] = pair\n",
        "\n",
        "            for k in range(len(corpus)):\n",
        "                word = corpus[k]\n",
        "                for i in range(len(word) - 1):\n",
        "                    curr_pair = (word[i], word[i+1])\n",
        "                    if curr_pair == max_pair:\n",
        "                        corpus[k] = word[:i] + [word[i] + word[i+1]] + word[i+2:]\n",
        "\n",
        "        # create a mapping between tokens and integers\n",
        "        for i, token in enumerate(self.vocab):\n",
        "            self.stoi[token] = i\n",
        "            self.itos[i] = token\n",
        "\n",
        "    def tokenize(self, text: str) -> list[str]:\n",
        "        '''\n",
        "        Encodes text into a list of tokens based on previously generated\n",
        "        set of rules\n",
        "\n",
        "            Parameters:\n",
        "                text (str): plain text to encode\n",
        "\n",
        "            Returns:\n",
        "                encoded_text (list[str]): list of tokens from generated vocab\n",
        "        '''\n",
        "        text = [list(word) for word in text.split(\" \")]\n",
        "        [word.insert(0, self.vocab[0]) for word in text]\n",
        "\n",
        "        for pair, merged in self.merge_rule.items():\n",
        "            for k in range(len(text)):\n",
        "                word = text[k]\n",
        "                for i in range(len(word) - 1):\n",
        "                    curr_pair = (word[i], word[i+1])\n",
        "                    if curr_pair == pair:\n",
        "                        text[k] = word[:i] + [word[i] + word[i+1]] + word[i+2:]\n",
        "\n",
        "        return [token for word in text for token in word]\n",
        "\n",
        "    def detokenize(self, tokens: list[str]) -> str:\n",
        "        '''\n",
        "        Decodes list of tokens into a plain text\n",
        "\n",
        "            Parameters:\n",
        "                tokens (list[str]): sequence of tokens\n",
        "\n",
        "            Returns:\n",
        "                text (str): decoded text\n",
        "        '''\n",
        "        text = \"\".join(tokens)\n",
        "        return text.replace('<s>', ' ')"
      ],
      "metadata": {
        "id": "hVty3EQxu1_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = [\"In the realm of modern education, the impact of technology has been nothing short of revolutionary. Gone are the days of traditional blackboards and chalk; instead, interactive whiteboards and digital tablets have become commonplace in classrooms around the world. This technological revolution has not only transformed the way students learn but has also empowered educators with innovative tools to engage and inspire their students. With the advent of online learning platforms and virtual classrooms, geographical barriers have been shattered, allowing students to access quality education from anywhere with an internet connection. Furthermore, educational apps and software have revolutionized the learning experience, making it more interactive, personalized, and adaptive to individual student needs. From gamified learning modules to virtual reality simulations, technology has opened up new avenues for experiential and immersive learning, enabling students to grasp complex concepts with greater ease and retention. Moreover, the integration of artificial intelligence and machine learning algorithms has enabled educators to analyze vast amounts of student data, providing valuable insights into learning patterns and identifying areas where students may need additional support. Additionally, the rise of educational technology startups and edtech companies has spurred innovation and competition in the education sector, leading to the development of cutting-edge tools and resources for both students and teachers. However, despite the myriad benefits of technology in education, it also presents its own set of challenges and concerns. Issues such as the digital divide, unequal access to technology, and concerns about data privacy and security remain pressing issues that need to be addressed. Moreover, there is a growing concern about the potential negative impact of excessive screen time on students' mental health and well-being. Therefore, while technology undoubtedly holds great promise for the future of education, it is essential to strike a balance between harnessing its potential benefits and addressing its inherent challenges in order to ensure that all students have access to quality education in the digital age.\"]\n",
        "\n",
        "bpe = BytePairEncoding()\n",
        "bpe.gen_vocab(sample_text, 200)"
      ],
      "metadata": {
        "id": "D1P6N56z4qi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bpe.vocab)\n",
        "print(bpe.merge_rule)\n",
        "print(\"------------\")\n",
        "\n",
        "text = \"Technology has revolutionized modern education by providing interactive tools and resources, but challenges such as the digital divide and concerns about data privacy and screen time need addressing for equitable access and student well-being.\"\n",
        "tokenized = bpe.tokenize(text)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgjkesKQ6ZBX",
        "outputId": "ac2ca9c9-686e-477c-a7d9-513c3826a5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'I', 'n', ' ', 't', 'h', 'e', 'r', 'a', 'l', 'm', 'o', 'f', 'd', 'u', 'c', 'i', ',', 'p', 'g', 'y', 's', 'b', 'v', '.', 'G', 'k', ';', 'w', 'T', 'W', 'q', 'F', 'z', 'x', 'M', 'A', '-', 'H', \"'\", '<s>a', '<s>t', 'in', 'en', 're', 'al', 'ti', 'on', 'he', 'ed', '<s>s', '<s>an', 'it', 'tion', 'ing', '<s>and', '<s>the', 'er', '<s>in', '<s>o', '<s>c', 'ent', 'es', 'ha', 'tu', '<s>to', '<s>of', 'ol', 'ec', '<s>b', 'or', 'ar', 've', 'le', '<s>ed', 'uc', 'ation', '<s>ha', '<s>educ', 'om', '<s>stu', '<s>stud', '<s>student', '<s>p', 'ac', '<s>d', '<s>w', '<s>re', '<s>m', '<s>education', 'ech', 'og', '<s>students', '<s>le', 'at', '<s>i', '<s>tech', '<s>techn', '<s>technol', '<s>technolog', '<s>n', 'ss', '<s>lear', '<s>learn', 'ern', '<s>technology', '<s>has', '<s>int', 'ou', '<s>learning', '<s>g', 'ess', '<s>it', 'ot', 'tive', 'ig', 'for', '<s>al', '<s>e', 'ith', 'ov', '<s>ad', 'ri', 'ore', '<s>ne', 'enti', '<s>be', 'ra', 'di', 'ab', 'ts', '<s>have', '<s>com', 'ow', '<s>en', '<s>v', 'cess', 'ual', 'ity', 'her', 'ce', 'ential', '<s>con', '<s>education,', '<s>im', '<s>rev', '<s>revol', '<s>revolu', '<s>revolution', 'ay', 'dition', 'ditional', 'ds', '<s>ch', '<s>chal', '<s>ins', 'ad', '<s>dig', '<s>digit', '<s>digital', 'hi', '<s>on', 'ly', 'red', '<s>with', 'ge', 'ir', 'ers', 'hat', '<s>ac', '<s>access', 'qual', 'rom', 'et', 'ur', '<s>ex', 'id', '<s>need', 's.', '<s>u', 'ues', '<s>for', '<s>comp', 'as', 'er,', 'ef', '<s>its', '<s>conc', '<s>concern', 'ress', '<s>I', '<s>real', '<s>mo', '<s>mod', '<s>imp', '<s>impac', '<s>impact', '<s>been', '<s>not', 'ort']\n",
            "{('<s>', 'a'): '<s>a', ('<s>', 't'): '<s>t', ('i', 'n'): 'in', ('e', 'n'): 'en', ('r', 'e'): 're', ('a', 'l'): 'al', ('t', 'i'): 'ti', ('o', 'n'): 'on', ('h', 'e'): 'he', ('e', 'd'): 'ed', ('<s>', 's'): '<s>s', ('<s>a', 'n'): '<s>an', ('i', 't'): 'it', ('ti', 'on'): 'tion', ('in', 'g'): 'ing', ('<s>an', 'd'): '<s>and', ('<s>t', 'he'): '<s>the', ('e', 'r'): 'er', ('<s>', 'in'): '<s>in', ('<s>', 'o'): '<s>o', ('<s>', 'c'): '<s>c', ('en', 't'): 'ent', ('e', 's'): 'es', ('h', 'a'): 'ha', ('t', 'u'): 'tu', ('<s>t', 'o'): '<s>to', ('<s>o', 'f'): '<s>of', ('o', 'l'): 'ol', ('e', 'c'): 'ec', ('<s>', 'b'): '<s>b', ('o', 'r'): 'or', ('a', 'r'): 'ar', ('v', 'e'): 've', ('l', 'e'): 'le', ('<s>', 'ed'): '<s>ed', ('u', 'c'): 'uc', ('a', 'tion'): 'ation', ('<s>', 'ha'): '<s>ha', ('<s>ed', 'uc'): '<s>educ', ('o', 'm'): 'om', ('<s>s', 'tu'): '<s>stu', ('<s>stu', 'd'): '<s>stud', ('<s>stud', 'ent'): '<s>student', ('<s>', 'p'): '<s>p', ('a', 'c'): 'ac', ('<s>', 'd'): '<s>d', ('<s>', 'w'): '<s>w', ('<s>', 're'): '<s>re', ('<s>', 'm'): '<s>m', ('<s>educ', 'ation'): '<s>education', ('ec', 'h'): 'ech', ('o', 'g'): 'og', ('<s>student', 's'): '<s>students', ('<s>', 'le'): '<s>le', ('a', 't'): 'at', ('<s>', 'i'): '<s>i', ('<s>t', 'ech'): '<s>tech', ('<s>tech', 'n'): '<s>techn', ('<s>techn', 'ol'): '<s>technol', ('<s>technol', 'og'): '<s>technolog', ('<s>', 'n'): '<s>n', ('s', 's'): 'ss', ('<s>le', 'ar'): '<s>lear', ('<s>lear', 'n'): '<s>learn', ('er', 'n'): 'ern', ('<s>technolog', 'y'): '<s>technology', ('<s>ha', 's'): '<s>has', ('<s>in', 't'): '<s>int', ('o', 'u'): 'ou', ('<s>learn', 'ing'): '<s>learning', ('<s>', 'g'): '<s>g', ('es', 's'): 'ess', ('<s>', 'it'): '<s>it', ('o', 't'): 'ot', ('ti', 've'): 'tive', ('i', 'g'): 'ig', ('f', 'or'): 'for', ('<s>a', 'l'): '<s>al', ('<s>', 'e'): '<s>e', ('it', 'h'): 'ith', ('o', 'v'): 'ov', ('<s>a', 'd'): '<s>ad', ('r', 'i'): 'ri', ('o', 're'): 'ore', ('<s>n', 'e'): '<s>ne', ('en', 'ti'): 'enti', ('<s>b', 'e'): '<s>be', ('r', 'a'): 'ra', ('d', 'i'): 'di', ('a', 'b'): 'ab', ('t', 's'): 'ts', ('<s>ha', 've'): '<s>have', ('<s>c', 'om'): '<s>com', ('o', 'w'): 'ow', ('<s>', 'en'): '<s>en', ('<s>', 'v'): '<s>v', ('c', 'ess'): 'cess', ('u', 'al'): 'ual', ('it', 'y'): 'ity', ('he', 'r'): 'her', ('c', 'e'): 'ce', ('enti', 'al'): 'ential', ('<s>c', 'on'): '<s>con', ('<s>education', ','): '<s>education,', ('<s>i', 'm'): '<s>im', ('<s>re', 'v'): '<s>rev', ('<s>rev', 'ol'): '<s>revol', ('<s>revol', 'u'): '<s>revolu', ('<s>revolu', 'tion'): '<s>revolution', ('a', 'y'): 'ay', ('di', 'tion'): 'dition', ('dition', 'al'): 'ditional', ('d', 's'): 'ds', ('<s>c', 'h'): '<s>ch', ('<s>ch', 'al'): '<s>chal', ('<s>in', 's'): '<s>ins', ('a', 'd'): 'ad', ('<s>d', 'ig'): '<s>dig', ('<s>dig', 'it'): '<s>digit', ('<s>digit', 'al'): '<s>digital', ('h', 'i'): 'hi', ('<s>', 'on'): '<s>on', ('l', 'y'): 'ly', ('re', 'd'): 'red', ('<s>w', 'ith'): '<s>with', ('g', 'e'): 'ge', ('i', 'r'): 'ir', ('er', 's'): 'ers', ('ha', 't'): 'hat', ('<s>a', 'c'): '<s>ac', ('<s>ac', 'cess'): '<s>access', ('q', 'ual'): 'qual', ('r', 'om'): 'rom', ('e', 't'): 'et', ('u', 'r'): 'ur', ('<s>e', 'x'): '<s>ex', ('i', 'd'): 'id', ('<s>ne', 'ed'): '<s>need', ('s', '.'): 's.', ('<s>', 'u'): '<s>u', ('u', 'es'): 'ues', ('<s>', 'for'): '<s>for', ('<s>com', 'p'): '<s>comp', ('a', 's'): 'as', ('er', ','): 'er,', ('e', 'f'): 'ef', ('<s>it', 's'): '<s>its', ('<s>con', 'c'): '<s>conc', ('<s>conc', 'ern'): '<s>concern', ('re', 'ss'): 'ress', ('<s>', 'I'): '<s>I', ('<s>re', 'al'): '<s>real', ('<s>m', 'o'): '<s>mo', ('<s>mo', 'd'): '<s>mod', ('<s>im', 'p'): '<s>imp', ('<s>imp', 'ac'): '<s>impac', ('<s>impac', 't'): '<s>impact', ('<s>be', 'en'): '<s>been', ('<s>n', 'ot'): '<s>not', ('or', 't'): 'ort'}\n",
            "------------\n",
            "['<s>', 'T', 'ech', 'n', 'ol', 'og', 'y', '<s>has', '<s>revolution', 'i', 'z', 'ed', '<s>mod', 'ern', '<s>education', '<s>b', 'y', '<s>p', 'r', 'ov', 'id', 'ing', '<s>int', 'er', 'ac', 'tive', '<s>to', 'ol', 's', '<s>and', '<s>re', 's', 'ou', 'r', 'c', 'es', ',', '<s>b', 'u', 't', '<s>chal', 'l', 'en', 'g', 'es', '<s>s', 'uc', 'h', '<s>a', 's', '<s>the', '<s>digital', '<s>d', 'i', 'v', 'id', 'e', '<s>and', '<s>concern', 's', '<s>a', 'b', 'ou', 't', '<s>d', 'at', 'a', '<s>p', 'ri', 'v', 'ac', 'y', '<s>and', '<s>s', 'c', 're', 'en', '<s>t', 'i', 'm', 'e', '<s>need', '<s>ad', 'd', 'ress', 'ing', '<s>for', '<s>e', 'q', 'u', 'it', 'ab', 'le', '<s>access', '<s>and', '<s>student', '<s>w', 'e', 'l', 'l', '-', 'b', 'e', 'ing', '.']\n"
          ]
        }
      ]
    }
  ]
}